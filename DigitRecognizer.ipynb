{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates a tensorflow `tf.data.Dataset` object from images on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "image_dims = (28, 28, 1)\n",
    "batchsize = 64\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training, data_validation = sklearn.model_selection.train_test_split(\n",
    "    pd.read_csv(\"train.csv\"), test_size=0.2)\n",
    "data_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X_training = data_training.drop(\"label\", axis=1).values\n",
    "X_validation = data_validation.drop(\"label\", axis=1).values\n",
    "X_test = data_test.values.reshape((-1,) + image_dims)\n",
    "\n",
    "y_training = data_training[\"label\"]\n",
    "y_validation = data_validation[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_data(image, label):\n",
    "    \"\"\"\n",
    "    Converts 1D array into 2D matrix and normalized entries\n",
    "    \"\"\"\n",
    "    image = tf.reshape(image, image_dims)\n",
    "    image = tf.dtypes.cast(image, tf.float64)\n",
    "    min_val = tf.reduce_min(image)\n",
    "    max_val = tf.reduce_max(image)\n",
    "    image = (image - min_val) / (max_val - min_val)\n",
    "    label = tf.one_hot(indices=label, depth=num_labels)\n",
    "    return image, label\n",
    "\n",
    "def preprocess_testing_data(image):\n",
    "    \"\"\"\n",
    "    Converts 1D array into 2D matrix and normalized entries\n",
    "    \"\"\"\n",
    "    image = tf.reshape(image, image_dims)\n",
    "    image = tf.dtypes.cast(image, tf.float64)\n",
    "    min_val = tf.reduce_min(image)\n",
    "    max_val = tf.reduce_max(image)\n",
    "    image = (image - min_val) / (max_val - min_val)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data, labels=None):\n",
    "    if labels is None:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "        dataset = dataset.map(preprocess_testing_data, num_parallel_calls=-1)\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "        dataset = dataset.shuffle(buffer_size=len(data))\n",
    "        dataset = dataset.map(preprocess_training_data, num_parallel_calls=-1)\n",
    "    dataset = dataset.batch(batchsize)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_training = make_dataset(X_training, y_training)\n",
    "dataset_validation = make_dataset(X_validation, y_validation)\n",
    "dataset_testing = make_dataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    input_layer = tf.keras.layers.Input(\n",
    "        shape=image_dims, name=\"Input\")\n",
    "    layer = tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=(3, 3), \n",
    "        activation=\"relu\", use_bias=True,\n",
    "        kernel_initializer=\"glorot_uniform\", \n",
    "        bias_initializer=\"glorot_uniform\")(input_layer)\n",
    "    layer = tf.keras.layers.Dropout(rate=0.3)(layer)\n",
    "    layer = tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=(3, 3), \n",
    "        activation=\"relu\", use_bias=True,\n",
    "        kernel_initializer=\"glorot_uniform\", \n",
    "        bias_initializer=\"glorot_uniform\")(layer)\n",
    "    layer = tf.keras.layers.Dropout(rate=0.3)(layer)\n",
    "    layer = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(layer)\n",
    "    layer = tf.keras.layers.Conv2D(\n",
    "        filters=128, kernel_size=(3, 3), \n",
    "        activation=\"relu\", use_bias=True,\n",
    "        kernel_initializer=\"glorot_uniform\", \n",
    "        bias_initializer=\"glorot_uniform\")(layer)\n",
    "    layer = tf.keras.layers.Dropout(rate=0.3)(layer)\n",
    "    layer = tf.keras.layers.Conv2D(\n",
    "        filters=128, kernel_size=(3, 3), \n",
    "        activation=\"relu\", use_bias=True,\n",
    "        kernel_initializer=\"glorot_uniform\", \n",
    "        bias_initializer=\"glorot_uniform\")(layer)\n",
    "    layer = tf.keras.layers.Dropout(rate=0.3)(layer)\n",
    "    layer = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(layer)\n",
    "    layer = tf.keras.layers.Flatten()(layer)\n",
    "    output_layer = tf.keras.layers.Dense(units=num_labels, activation=\"softmax\")(layer)\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=\"adam\", \n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 10, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 279,498\n",
      "Trainable params: 279,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0703 23:49:49.990462 140453180405504 deprecation.py:323] From /home/jan/anaconda3/envs/py3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525/525 [==============================] - 91s 174ms/step - loss: 0.2682 - acc: 0.9144 - val_loss: 0.1026 - val_acc: 0.9757\n",
      "Epoch 2/5\n",
      "525/525 [==============================] - 87s 166ms/step - loss: 0.0745 - acc: 0.9772 - val_loss: 0.0725 - val_acc: 0.9819\n",
      "Epoch 3/5\n",
      "525/525 [==============================] - 85s 161ms/step - loss: 0.0532 - acc: 0.9831 - val_loss: 0.0559 - val_acc: 0.9870\n",
      "Epoch 4/5\n",
      "525/525 [==============================] - 86s 163ms/step - loss: 0.0429 - acc: 0.9867 - val_loss: 0.0410 - val_acc: 0.9890\n",
      "Epoch 5/5\n",
      "525/525 [==============================] - 87s 165ms/step - loss: 0.0355 - acc: 0.9889 - val_loss: 0.0477 - val_acc: 0.9888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbd82747b00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset_training, epochs=5, validation_data=dataset_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(dataset_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(\n",
    "    data={\n",
    "        \"ImageId\": data_test.index+1, \n",
    "        \"Label\": np.argmax(pred, axis=1)})\n",
    "\n",
    "output.head()\n",
    "output.to_csv(\"TestPrediction.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
